import requests
from bs4 import BeautifulSoup
import pandas as pd








def scrape_data(url, apartment_type, date):
    # Fetch the HTML content
    response = requests.get(url)
    html_content = response.content

    # Parse the HTML using BeautifulSoup
    soup = BeautifulSoup(html_content, 'html.parser')

    # Find the table with class 'stats'
    table = soup.find('table', {'class': 'stats'})

    # Extract the table rows
    rows = []
    for tr in table.find('tbody').find_all('tr'):
        cells = tr.find_all('td')
        row = [cell.text.strip() for cell in cells]
        # Add 'EUR' and apartment type for each row
        row.extend(['EUR', apartment_type, date])
        rows.append(row)
    
    return rows





# URLs to scrape
url_two_bedroom = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=0&property_type_id%5B%5D=6&currency_id=4&date=2024-07-28"
url_three_bedroom = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=9&currency_id=4&date=2024-07-28"
url_house = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=14&currency_id=4&date=2024-07-28"
url_garage = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=20&currency_id=4&date=2024-07-28"
url_studio = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=13&currency_id=4&date=2024-07-28"
url_large_apartments = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=10&currency_id=4&date=2024-07-28"

# Scrape data from both URLs
rows_two_bedroom = scrape_data(url_two_bedroom, 'Двустаен апартамент', '2024-07-28')
rows_three_bedroom = scrape_data(url_тристаен, 'Тристаен апартамент', '2024-07-28')
rows_house = scrape_data(url_house, 'Къща', '2024-07-28')
rows_garage = scrape_data(url_garage, "Гараж, паркомясто", '2024-07-28')
rows_studio = scrape_data(url_studio, "Ателие, таван, студио", '2024-07-28')
rows_large_apartments = scrape_data(url_large_apartments, "Многостаен апартамемент", '2024-07-28')

# Combine the rows
all_rows = rows_two_bedroom + rows_three_bedroom + rows_house + rows_garage + rows_studio + rows_large_apartments

# Specify the headers manually since we know the structure
headers = ['Район', 'Цена', 'Цена / кв.м.', 'Валута', 'Тип Апартамент', 'Дата']

# Convert the data to a pandas DataFrame
df = pd.DataFrame(all_rows, columns=headers)

# Save the DataFrame to a CSV file
df.to_csv('data/property_prices_july_2024.csv', index=False)

print('Data scraping complete. Saved to property_prices.csv')






# URLs to scrape
url_two_bedroom = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=0&property_type_id%5B%5D=6&currency_id=4&date=2023-07-28"
url_three_bedroom = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=9&currency_id=4&date=2023-07-28"
url_house = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=14&currency_id=4&date=2023-07-28"
url_garage = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=20&currency_id=4&date=2023-07-28"
url_studio = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=13&currency_id=4&date=2023-07-28"
url_large_apartments = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=10&currency_id=4&date=2023-07-28"

# Scrape data from both URLs
rows_two_bedroom = scrape_data(url_two_bedroom, 'Двустаен апартамент', '2023-07-28')
rows_three_bedroom = scrape_data(url_тристаен, 'Тристаен апартамент', '2023-07-28')
rows_house = scrape_data(url_house, 'Къща', '2023-07-28')
rows_garage = scrape_data(url_garage, "Гараж, паркомясто", '2023-07-28')
rows_studio = scrape_data(url_studio, "Ателие, таван, студио", '2023-07-28')
rows_large_apartments = scrape_data(url_large_apartments, "Многостаен апартамемент", '2023-07-28')

# Combine the rows
all_rows = rows_two_bedroom + rows_three_bedroom + rows_house + rows_garage + rows_studio + rows_large_apartments

# Specify the headers manually since we know the structure
headers = ['Район', 'Цена', 'Цена / кв.м.', 'Валута', 'Тип Апартамент', 'Дата']

# Convert the data to a pandas DataFrame
df = pd.DataFrame(all_rows, columns=headers)

# Save the DataFrame to a CSV file
df.to_csv('data/property_prices_july_2023.csv', index=False)

print('Data scraping complete. Saved to property_prices.csv')






# URLs to scrape
url_two_bedroom = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=0&property_type_id%5B%5D=6&currency_id=4&date=2022-07-28"
url_three_bedroom = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=9&currency_id=4&date=2022-07-28"
url_house = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=14&currency_id=4&date=2022-07-28"
url_garage = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=20&currency_id=4&date=2022-07-28"
url_studio = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=13&currency_id=4&date=2022-07-28"
url_large_apartments = "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=&property_type_id%5B%5D=10&currency_id=4&date=2022-07-28"

# Scrape data from both URLs
rows_two_bedroom = scrape_data(url_two_bedroom, 'Двустаен апартамент', '2022-07-28')
rows_three_bedroom = scrape_data(url_тристаен, 'Тристаен апартамент', '2022-07-28')
rows_house = scrape_data(url_house, 'Къща', '2022-07-28')
rows_garage = scrape_data(url_garage, "Гараж, паркомясто", '2022-07-28')
rows_studio = scrape_data(url_studio, "Ателие, таван, студио", '2022-07-28')
rows_large_apartments = scrape_data(url_large_apartments, "Многостаен апартамемент", '2022-07-28')

# Combine the rows
all_rows = rows_two_bedroom + rows_three_bedroom + rows_house + rows_garage + rows_studio + rows_large_apartments

# Specify the headers manually since we know the structure
headers = ['Район', 'Цена', 'Цена / кв.м.', 'Валута', 'Тип Апартамент', 'Дата']

# Convert the data to a pandas DataFrame
df = pd.DataFrame(all_rows, columns=headers)

# Save the DataFrame to a CSV file
df.to_csv('data/property_prices_july_2022.csv', index=False)

print('Data scraping complete. Saved to property_prices.csv')






# Function to fetch and parse the HTML content
def fetch_html(url):
    response = requests.get(url)
    return response.content

# Function to parse the table and extract data
def parse_table(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    table = soup.find('table', {'id': 'tableStats'})
    
    headers = ['Район', 'Едностайни - цена', 'Едностайни - €/кв.м', 
               'Двустайни - цена', 'Двустайни - €/кв.м', 
               'Тристайни - цена', 'Тристайни - €/кв.м', 'Общо - €/кв.м']
    
    rows = []
    
    for tr in table.find_all('tr')[2:]:  # Skip the first two header rows
        cells = tr.find_all('td')
        row = []
        
        for cell in cells:
            text = cell.get_text(strip=True).replace('\xa0', ' ')
            if text == '-':
                text = None
            row.append(text)
        
        if len(row) == 12:
            row = [row[0], row[2], row[3], row[5], row[6], row[8], row[9], row[11]]
            rows.append(row)
    
    df = pd.DataFrame(rows, columns=headers)
    return df

# URLs of the webpages to scrape
urls = [
    "https://www.imot.bg/pcgi/imot.cgi?act=14&",
    # "https://www.imoti.net/bg/sredni-ceni?ad_type_id=2&city_id=1&region_id=0&property_type_id%5B%5D=9&currency_id=4&date=2024-07-28"
]

# Fetch and parse the HTML content from both URLs
dfs = []
for url in urls:
    html_content = fetch_html(url)
    df = parse_table(html_content)
    dfs.append(df)

# Concatenate all DataFrames and save to a CSV file
final_df = pd.concat(dfs, ignore_index=True)
final_df.to_csv('data2/property_prices.csv', index=False, encoding='utf-8')

print('Data scraping complete. Saved to property_prices.csv')



import requests
from bs4 import BeautifulSoup
import pandas as pd

# Function to fetch and parse the HTML content
def fetch_html(url):
    response = requests.get(url)
    return response.content

# Function to parse the table and extract data
def parse_table(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    table = soup.find('table', {'id': 'tableStats'})
    
    headers = ['Район', 'Едностайни - цена', 'Едностайни - €/кв.м', 
               'Двустайни - цена', 'Двустайни - €/кв.м', 
               'Тристайни - цена', 'Тристайни - €/кв.м', 'Общо - €/кв.м', 'Дата']
    

    
    # Extract date
    date_element = soup.find('td', {'style': 'border-top: 1px solid #900;padding-top: 20px;font-size: 13px;padding-bottom: 5px;'})
    date_text = date_element.get_text(strip=True)
    date_str = date_text.split("от")[-1].split(" г.")[0].strip()  # Extract the date in the format '30.7.2024'
    

    rows = []
    
    for tr in table.find_all('tr')[2:]:  # Skip the first two header rows
        cells = tr.find_all('td')
        row = []
        
        for cell in cells:
            text = cell.get_text(strip=True).replace('\xa0', ' ')
            if text == '-':
                text = None
            row.append(text)
        
        if len(row) == 12:
            row = [row[0], row[2], row[3], row[5], row[6], row[8], row[9], row[11], date_str]
            rows.append(row)
    
    df = pd.DataFrame(rows, columns=headers)
    return df

# URLs of the webpages to scrape
urls = [
    "https://www.imot.bg/pcgi/imot.cgi?act=14&",
    "https://www.imot.bg/pcgi/imot.cgi?act=14&pn=0&town=%D1%EE%F4%E8%FF&year=2023&date=25.7.2023",
    "https://www.imot.bg/pcgi/imot.cgi?act=14&pn=0&town=%D1%EE%F4%E8%FF&year=2022&date=26.7.2022",
]

# Fetch and parse the HTML content from both URLs
dfs = []
for url in urls:
    html_content = fetch_html(url)
    df = parse_table(html_content)
    dfs.append(df)

# Concatenate all DataFrames and save to a CSV file
final_df = pd.concat(dfs, ignore_index=True)
final_df.fillna('Null', inplace=True)
final_df.to_csv('data2/property_prices.csv', index=False, encoding='utf-8')

print('Data scraping complete. Saved to property_prices.csv')




